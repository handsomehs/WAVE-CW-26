# Report 可用内容素材（随实现逐步补充）

## Section 1: 理论性能上限估算（A100 Roofline）
- **核心公式**：
  - 每点 FLOPs（约 12–14，按实际 kernel 统计）
  - 每点字节流量（朴素 80–88 B；优化目标 32–40 B）
  - 带宽上限 SU/s ≈ HBM_BW / BytesPerSite
  - 计算上限 SU/s ≈ PeakFLOPs / FLOPsPerSite
  - 理论上限 = min(带宽上限, 计算上限)
- **需要补充的数据**：
  - A100 SXM4 40GB 官方带宽/峰值（引用 Nvidia 文档）
  - 实际 kernel 的 FLOPs/点与字节/点核算表

## Section 2: 实现选择与性能证据
- **设计选择（示例）**：
  - 数据常驻 + 指针轮换（避免每步 memcpy）。
  - 内/边界分核（减少分支）。
  - `cs2`/`damp` 降维或预计算（降带宽）。
  - OpenMP 与 CUDA 的并行映射策略。
- **性能证据（待补充）**：
  - SU/s 随规模变化曲线（32–1000）。
  - Nsight Systems 时间线截图（核/拷贝重叠）。
  - Nsight Compute 指标表（DRAM 吞吐、占用、分支发散）。

## Section 3: CUDA vs OpenMP 对比
- **维度**：
  - 性能：大规模带宽饱和度、小规模启动开销。
  - 开发成本：代码复杂度、调优空间。
  - 可移植性：OpenMP 的可迁移性 vs CUDA 的生态与性能上限。
- **建议结论模板**：
  - 若以性能为优先 → CUDA。
  - 若以可移植与开发效率为优先 → OpenMP。
  - 提供量化证据支撑结论。

## 附：可直接引用/改写的技术描述模板
- “该 stencil 的算术强度低于 0.2 FLOP/Byte，因此性能上限由内存带宽决定。”
- “分离内域/边界核后，内域去除分支，提升了 warp 执行效率。”
- “通过数据常驻与指针轮换，避免了每步 H2D/D2H 开销，仅在输出点回传。”
- “OpenMP offload 在开发效率上更直接，但细粒度性能调优不如 CUDA 灵活。”
